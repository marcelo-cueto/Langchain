{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRUEBA DE REQUEST A WIKIPEDIA API\n",
    "\n",
    "import requests\n",
    "\n",
    "# URL de la API de Wikipedia (para obtener datos en formato JSON)\n",
    "url = \"https://es.wikipedia.org/w/api.php\"\n",
    "\n",
    "# Parámetros de la solicitud para obtener el resumen de una página\n",
    "params = {\n",
    "    \"action\": \"query\",\n",
    "    \"prop\": \"extracts\",\n",
    "    \"exintro\": True,\n",
    "    \"titles\": \"Albert Einstein\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "# Hacer la solicitud GET a la API de Wikipedia\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Comprobar si la respuesta fue exitosa\n",
    "if response.status_code == 200:\n",
    "    # Obtener los datos en formato JSON\n",
    "    data = response.json()\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    for page_id, page_data in pages.items():\n",
    "        print(f\"Título: {page_data['title']}\")\n",
    "        print(f\"Extracto: {page_data['extract']}\")\n",
    "else:\n",
    "    print(f\"Error al conectarse a Wikipedia: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRUEBA DE SOLICITUD API A OLLAMA MODEL EN DOCKER\n",
    "\n",
    "import requests\n",
    "\n",
    "# URL correcta para interactuar con el API de Ollama\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Datos de la solicitud\n",
    "data = {\n",
    "    \"model\": \"llama3.2\",  # Cambia al modelo que deseas usar\n",
    "    \"prompt\": \"Responde a la siguiente pregunta en formato JSON: ¿Cuál es el significado de la vida?\",\n",
    "    #\"format\": \"json\",  # El formato de la respuesta\n",
    "    \"stream\": False  # Opcional, pero si no quieres respuestas por streaming\n",
    "}\n",
    "# Enviar la solicitud POST\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Verificar la respuesta\n",
    "if response.status_code == 200:\n",
    "    print(\"Respuesta de Ollama:\", response.json())\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL para la API de Instant Answers de DuckDuckGo\n",
    "url = \"https://api.duckduckgo.com/\"\n",
    "\n",
    "# Parámetros de la solicitud\n",
    "params = {\n",
    "    \"q\": \"¿Cuál es el significado de la vida?\",  # La pregunta que deseas hacer\n",
    "    \"format\": \"json\",  # Formato de respuesta\n",
    "    \"RelatedTopics\":True, \n",
    "    #\"no_html\": 1,  # Opcional, para evitar HTML en la respuesta\n",
    "    #\"skip_disambig\": 1  # Opcional, para evitar respuestas de desambiguación\n",
    "}\n",
    "\n",
    "# Enviar la solicitud GET\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Verificar la respuesta\n",
    "if response.status_code == 200:\n",
    "    print(\"Respuesta de DuckDuckGo:\", response.json())\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://openlibrary.org/search.json\"\n",
    "params = {\n",
    "    \"title\": \"The Meaning of Life\"  # Cambia al título o tema deseado\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "if response.status_code == 200:\n",
    "    books = response.json()[\"docs\"]\n",
    "    for book in books[:5]:  # Muestra los primeros 5 resultados\n",
    "        print(\"Título:\", book.get(\"title\"), \"| Autor:\", book.get(\"author_name\"))\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.wolframalpha.com/v1/result\"\n",
    "params = {\n",
    "    \"appid\": \"VLVTJ3-KW3TQPUGVG\",  # Necesitas una clave API gratuita de Wolfram Alpha\n",
    "    \"i\": \"Eres un experto en geografía y tienes un conocimiento detallado de ciudades y países. Responde de manera detallada y en formato JSON: ¿Dónde queda Buenos Aires?\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "if response.status_code == 200:\n",
    "    print(\"Respuesta de Wolfram Alpha:\", response.text)\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LANGCHAIN OLLAMA MODEL:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PROBANDO AGREGADO DE HERRAMIENTAS COMO WIKIPEDIA Y DUCKDUCKGO\n",
    "\n",
    "El código ahora también utiliza MariaDB como almacenamiento de memoria y LlamaIndex para una indexación rápida y búsqueda de consultas previas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import mariadb\n",
    "import json\n",
    "import webbrowser\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar stopwords de NLTK si no están instaladas\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Definición del modelo Ollama LLM (usando Ollama en Docker)\n",
    "class OllamaLLM:\n",
    "    def __init__(self, api_url=\"http://localhost:11434/api/generate\", model=\"llama3.2:1b\"):\n",
    "        self.api_url = api_url\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, prompt: str, stream: bool = False):\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        try:\n",
    "            logging.debug(f\"Enviando solicitud a Ollama con el modelo {self.model} y el prompt '{prompt}'\")\n",
    "            response = requests.post(self.api_url, json=data)\n",
    "            response.raise_for_status()  # Lanzar excepción si hay error HTTP\n",
    "            result = response.json().get('response', '')\n",
    "            logging.debug(f\"Respuesta de Ollama: {result}\")\n",
    "            \n",
    "            if result.strip():  # Asegurarse de que la respuesta no esté vacía\n",
    "                return result\n",
    "            else:\n",
    "                logging.error(\"Ollama devolvió una respuesta vacía.\")\n",
    "                return None\n",
    "        except requests.Timeout:\n",
    "            logging.error(\"Tiempo de espera agotado para Ollama.\")\n",
    "            return None\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error en Ollama: {e}\")\n",
    "            return None\n",
    "\n",
    "# Clase de memoria utilizando MariaDB\n",
    "class MariaDBMemory:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.connection = mariadb.connect(\n",
    "                host='host.docker.internal',  # Cambiado para que coincida con el contenedor\n",
    "                port=49154,  # Puerto especificado en la cadena de conexión\n",
    "                user='root',\n",
    "                password='mariadbpw',\n",
    "                database='ollama_memory'  # Asegúrate de que esta base de datos exista\n",
    "            )\n",
    "            self.connection.autocommit = True\n",
    "            print(f\"SELFCONECTION: {self.connection}\")\n",
    "            logging.info(\"Conexión a MariaDB exitosa\")\n",
    "            self.create_table()  # Crear la tabla si no existe\n",
    "        except mariadb.Error as err:\n",
    "            logging.error(f\"Error al conectar a MariaDB: {err}\")\n",
    "            self.connection = None\n",
    "\n",
    "    def create_table(self):\n",
    "        if self.connection is None:\n",
    "            logging.error(\"No se puede crear la tabla: la conexión a MariaDB no fue establecida.\")\n",
    "            return\n",
    "        cursor = self.connection.cursor()\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS memory (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    prompt TEXT NOT NULL,\n",
    "                    response LONGTEXT NOT NULL,  # Cambiado a LONGTEXT para respuestas largas\n",
    "                    score INT DEFAULT 0\n",
    "                );\n",
    "            \"\"\")\n",
    "            logging.info(\"Tabla 'memory' asegurada.\")\n",
    "        except mariadb.Error as err:\n",
    "            logging.error(f\"Error al crear la tabla: {err}\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "    def save_memory(self, prompt: str, response: dict, score: int):\n",
    "        # Verificar que alguna de las respuestas no esté vacía\n",
    "        valid_response = any(response.get(key) for key in ['ollama', 'wikipedia', 'duckduckgo'])\n",
    "\n",
    "        if not valid_response:  # Si todas las respuestas son vacías o None\n",
    "            logging.warning(\"No se guarda la respuesta vacía en la base de datos.\")\n",
    "            return\n",
    "\n",
    "        if self.connection is None:\n",
    "            logging.error(\"No se puede guardar en memoria: la conexión a MariaDB no fue establecida.\")\n",
    "            return\n",
    "        cursor = self.connection.cursor()\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO memory (prompt, response, score)\n",
    "                VALUES (%s, %s, %s);\n",
    "            \"\"\", (prompt, str(response), score))  # Convertir el diccionario en cadena para almacenar\n",
    "            logging.info(f\"Memoria guardada: {prompt} con puntaje {score}\")\n",
    "        except mariadb.Error as err:\n",
    "            logging.error(f\"Error al guardar en memoria: {err}\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "    def retrieve_memory(self, prompt: str):\n",
    "        if self.connection is None:\n",
    "            logging.error(\"No se puede recuperar de memoria: la conexión a MariaDB no fue establecida.\")\n",
    "            return None\n",
    "        cursor = self.connection.cursor()\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT response, score FROM memory\n",
    "                WHERE prompt = %s;\n",
    "            \"\"\", (prompt,))\n",
    "            result = cursor.fetchone()\n",
    "            return result if result else None\n",
    "        except mariadb.Error as err:\n",
    "            logging.error(f\"Error al recuperar de memoria: {err}\")\n",
    "            return None\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "# Clase para obtener información de Wikipedia utilizando requests\n",
    "class WikipediaTool:\n",
    "    def __init__(self, language='es'):\n",
    "        self.api_url = f\"https://{language}.wikipedia.org/w/api.php\"\n",
    "\n",
    "    def get_summary(self, query: str):\n",
    "        \"\"\"Devuelve el resumen del artículo de Wikipedia utilizando requests.\"\"\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exintro\": True,\n",
    "            \"titles\": query,\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        try:\n",
    "            logging.info(f\"Haciendo solicitud a Wikipedia para: {query}\")\n",
    "            response = requests.get(self.api_url, params=params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                pages = data[\"query\"][\"pages\"]\n",
    "                for page_id, page_data in pages.items():\n",
    "                    logging.info(f\"Título encontrado: {page_data['title']}\")\n",
    "                    return page_data.get('extract', \"No se encontró extracto disponible.\")\n",
    "            else:\n",
    "                logging.error(f\"Error al conectarse a Wikipedia: {response.status_code}\")\n",
    "                return None\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error en la solicitud a Wikipedia: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_full_article(self, query: str):\n",
    "        \"\"\"Devuelve el contenido completo del artículo de Wikipedia utilizando requests.\"\"\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"extracts\",\n",
    "            \"titles\": query,\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        try:\n",
    "            logging.info(f\"Haciendo solicitud a Wikipedia para: {query}\")\n",
    "            response = requests.get(self.api_url, params=params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                pages = data[\"query\"][\"pages\"]\n",
    "                for page_id, page_data in pages.items():\n",
    "                    logging.info(f\"Título encontrado: {page_data['title']}\")\n",
    "                    return page_data.get('extract', \"No se encontró contenido disponible.\")\n",
    "            else:\n",
    "                logging.error(f\"Error al conectarse a Wikipedia: {response.status_code}\")\n",
    "                return None\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error en la solicitud a Wikipedia: {e}\")\n",
    "            return None\n",
    "\n",
    "# Clase para obtener información de DuckDuckGo\n",
    "class DuckDuckGoTool:\n",
    "    def __init__(self):\n",
    "        self.api_url = \"https://api.duckduckgo.com/\"\n",
    "\n",
    "    def get_answer(self, query: str):\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'format': 'json',\n",
    "            \"kl\": \"es-es\"\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(self.api_url, params=params)\n",
    "            response.raise_for_status()  # Lanzar excepción si hay error\n",
    "            data = response.json()\n",
    "            if 'AbstractText' in data and data['AbstractText']:\n",
    "                return data['AbstractText']\n",
    "            else:\n",
    "                return None  # Devuelve None si no encuentra nada\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error en DuckDuckGo: {e}\")\n",
    "            return None\n",
    "\n",
    "# Extender el modelo Ollama para que use la memoria en MariaDB\n",
    "class OllamaWithMemory:\n",
    "    def __init__(self, model: OllamaLLM, memory: MariaDBMemory):\n",
    "        self.model = model\n",
    "        self.memory = memory\n",
    "\n",
    "    def generate_with_memory(self, prompt: str):\n",
    "        # Intentar recuperar de la memoria\n",
    "        memory_response = self.memory.retrieve_memory(prompt)\n",
    "        if memory_response:\n",
    "            logging.info(\"Respuesta encontrada en memoria.\")\n",
    "            return memory_response\n",
    "\n",
    "        # Si no está en memoria, generar nueva respuesta\n",
    "        logging.info(\"Generando nueva respuesta...\")\n",
    "        response = self.model.generate(prompt)\n",
    "        if response:  # Solo guarda si hay una respuesta válida\n",
    "            return response, 1  # Retornar respuesta con puntaje 1 si solo Ollama respondió\n",
    "        return None, 0\n",
    "\n",
    "# Integración de Ollama, Wikipedia y DuckDuckGo con memoria y sistema de puntaje\n",
    "class OllamaWithMemoryAndTools(OllamaWithMemory):\n",
    "    def __init__(self, model: OllamaLLM, memory: MariaDBMemory, tools: dict):\n",
    "        super().__init__(model=model, memory=memory)\n",
    "        self.tools = tools\n",
    "        self.stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "    def clean_question(self, question: str):\n",
    "        \"\"\"Limpia la pregunta eliminando stopwords y caracteres innecesarios.\"\"\"\n",
    "        # Convertir a minúsculas y eliminar caracteres especiales\n",
    "        question_cleaned = re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚñÑ\\s]', '', question.lower())\n",
    "        \n",
    "        # Dividir en palabras y eliminar stopwords\n",
    "        words = question_cleaned.split()\n",
    "        important_words = [word for word in words if word not in self.stop_words]\n",
    "        \n",
    "        # Unir las palabras importantes de nuevo en una cadena\n",
    "        return ' '.join(important_words)\n",
    "\n",
    "    def ask(self, question: str, full_article=True):\n",
    "        # Generar respuesta utilizando memoria y herramientas\n",
    "        ollama_response, score = self.generate_with_memory(question)\n",
    "        wikipedia_response = None\n",
    "        duckduckgo_response = None\n",
    "\n",
    "        # Limpiar la pregunta para extraer el tema\n",
    "        cleaned_topic = self.clean_question(question)\n",
    "        \n",
    "        # Intentar recuperar de memoria primero\n",
    "        memory_data = self.memory.retrieve_memory(cleaned_topic)\n",
    "        if memory_data:\n",
    "            logging.info(f\"Datos encontrados en memoria para: {cleaned_topic}\")\n",
    "            responses, stored_score = json.loads(memory_data[0]), memory_data[1]\n",
    "            \n",
    "            # Verificamos si falta alguna respuesta (Wikipedia o DuckDuckGo)\n",
    "            if not responses.get('wikipedia'):\n",
    "                logging.info(\"No se encontró respuesta de Wikipedia, intentando recuperarla...\")\n",
    "                if full_article:\n",
    "                    wikipedia_response = self.tools['wikipedia'].get_full_article(cleaned_topic)\n",
    "                else:\n",
    "                    wikipedia_response = self.tools['wikipedia'].get_summary(cleaned_topic)\n",
    "                if wikipedia_response:\n",
    "                    responses['wikipedia'] = wikipedia_response\n",
    "                    score += 1\n",
    "            \n",
    "            if not responses.get('duckduckgo'):\n",
    "                logging.info(\"No se encontró respuesta de DuckDuckGo, intentando recuperarla...\")\n",
    "                duckduckgo_response = self.tools['duckduckgo'].get_answer(cleaned_topic)\n",
    "                if duckduckgo_response:\n",
    "                    responses['duckduckgo'] = duckduckgo_response\n",
    "                    score += 1\n",
    "\n",
    "            # Si alguna respuesta faltaba y la recuperamos, actualizamos en memoria\n",
    "            if wikipedia_response or duckduckgo_response:\n",
    "                self.memory.save_memory(question, responses, score)\n",
    "\n",
    "            return responses, score\n",
    "\n",
    "        # Si no hay memoria, hacemos las consultas normalmente\n",
    "        if cleaned_topic:\n",
    "            logging.info(f\"Buscando en Wikipedia: {cleaned_topic}\")\n",
    "            if full_article:\n",
    "                wikipedia_response = self.tools['wikipedia'].get_full_article(cleaned_topic)  # Obtener artículo completo\n",
    "            else:\n",
    "                wikipedia_response = self.tools['wikipedia'].get_summary(cleaned_topic)  # Obtener solo el resumen\n",
    "            \n",
    "            if wikipedia_response:\n",
    "                logging.info(f\"Respuesta de Wikipedia recibida para {cleaned_topic}\")\n",
    "                logging.info(f\"RESPUESTA WIKIPEDIA: {wikipedia_response}\")\n",
    "                score += 1\n",
    "            else:\n",
    "                logging.error(f\"No se encontró información en Wikipedia para {cleaned_topic}\")\n",
    "        \n",
    "        duckduckgo_response = self.tools['duckduckgo'].get_answer(cleaned_topic if cleaned_topic else question)\n",
    "        if duckduckgo_response:\n",
    "            logging.info(f\"Respuesta de DuckDuckGo recibida para {cleaned_topic or question}\")\n",
    "            score += 1\n",
    "        else:\n",
    "            logging.error(f\"No se encontró información en DuckDuckGo para {cleaned_topic or question}\")\n",
    "\n",
    "        # Guardar todas las respuestas separadas\n",
    "        responses = {\n",
    "            \"ollama\": ollama_response,\n",
    "            \"wikipedia\": wikipedia_response,\n",
    "            \"duckduckgo\": duckduckgo_response\n",
    "        }\n",
    "\n",
    "        if ollama_response:\n",
    "            # Guardar solo si Ollama responde correctamente\n",
    "            self.memory.save_memory(question, responses, score)\n",
    "\n",
    "        return responses, score  # Retorna todas las respuestas por separado con puntaje\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso\n",
    "ollama_llm = OllamaLLM()\n",
    "mariadb_memory = MariaDBMemory()\n",
    "\n",
    "# Herramientas\n",
    "wikipedia_tool = WikipediaTool(language=\"es\")\n",
    "duckduckgo_tool = DuckDuckGoTool()\n",
    "\n",
    "tools = {\n",
    "    \"wikipedia\": wikipedia_tool,\n",
    "    \"duckduckgo\": duckduckgo_tool\n",
    "}\n",
    "\n",
    "# Crear la instancia de Ollama con memoria y herramientas\n",
    "ollama_with_memory_and_tools = OllamaWithMemoryAndTools(\n",
    "    model=ollama_llm, \n",
    "    memory=mariadb_memory, \n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "# Hacer una pregunta\n",
    "response = ollama_with_memory_and_tools.ask(\"¿Quién fue Albert Einstein?\")\n",
    "print(\"Respuesta:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Selecciona el modelo que deseas usar\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"  # O \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Mover el modelo a GPU si está disponible\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "# Función para generar texto\n",
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Usar la función\n",
    "prompt = \"¿Cuál es el futuro de la inteligencia artificial?\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
